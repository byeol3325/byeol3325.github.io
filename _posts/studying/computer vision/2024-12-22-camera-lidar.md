---
title:  "[Computer Vision] Lidar - Camera fusion 연구에 대해" 
categories: studying
tag: [cpp, studying, 3D, point cloud, computer vision, cv]
date: 2024-12-22
toc: true
use_math: true
sidebar:
    nav: "docs"
last_modified_at: 2024-12-22
---

최근에 너무 바빴어서(4개의 기업과 프로젝트를 진행하느라 ...) 오랜만에 글을 올리게 됐습니다. 최대한 기록을 남기고자 다시 올리려 합니다... 

이번 글은 새로운 연구 프로젝트에 앞서 공부하려고 기록용으로 남기고자 합니다. Camera sensor(image)와 Lidar(point cloud)가 어떻게 함께 이용되고 있으며 연구 방향, 최근 연구 트렌드, 대표적인 논문 등 다양한 것들을 다뤄보려 합니다. 😁


# Lidar와 Camera 개념과 특징

들어가기 앞서 Lidar와 Camera에 대해 간략하게 정리하겠습니다.


## Lidar란 무엇인가?

**Lidar(Light Detection and Ranging)**는 레이저 빛을 활용하여 물체와의 거리를 측정하는 센서 기술입니다. 레이저 펄스를 물체에 쏘아 반사되어 돌아오는 시간을 측정하고, 이를 통해 물체까지의 거리와 위치를 계산합니다. 이 데이터를 기반으로 고해상도의 **3D 포인트 클라우드(Point Cloud)**를 생성하며, 환경의 정밀한 공간 정보를 제공합니다.

Lidar의 주요 장점은 높은 해상도와 정밀도입니다. 3D 공간에서 객체의 모양, 크기, 위치를 세부적으로 파악할 수 있기 때문에 자율주행, 로봇공학, 드론, 건설 등 다양한 분야에서 사용됩니다. 

하지만 Lidar는 악천후(비, 안개, 눈) 환경에서 성능이 저하될 수 있으며, 상대적으로 높은 비용이 단점으로 꼽힙니다.


## Camera란 무엇인가?

**Camera**는 가시광선이나 근적외선을 수집하여 이미지를 생성하는 센서입니다. 광학 렌저를 통해 환경에서 반사된 빛을 센서에 집광한 뒤, 이를 전기 신호로 변환해 디지털 이미지 데이터를 제공합니다. 이러한 이미지는 RGB 데이터 형태로 표현되며, 사물의 색상, 텍스처, 모양과 같은 시각적 정보를 인식할 수 있습니다.

카메라는 다양한 이유로 가장 보편적이고 경제적인 센서로 자리 잡고 있습니다. 타 센서(Lidar, Radar)에 비해 상대적으로 크기가 작으며 저렴한 비용으로 고해상도 이미지를 제공하며, 객체 분류(Classification), 이미지 분할(Segmentation), 2D 및 3D 객체 탐지(Object Detection) 등 컴퓨터 비전 분야의 핵심 역할을 합니다. 특히, 카메라 데이터를 활용한 딥러닝 모델은 자율주행, 의료, 보안, 제조 등 다양한 산업에서 혁신적인 응용을 가능하게 하고 있습니다.

하지만 카메라의 경우, 깊이 정보(Depth)를 직접 제공하지 못하며, 카메라를 통해 이를 얻기 위해 추가적인 알고리즘(Stereo Vision, Depth Estimation)을 사용해야합니다. 또한, 빛의 양(어두운 환경)이나 날씨(비, 안개)에 따라 민감하고, 이러한 경우 정확도가 저하되며 정확한 주변 정보를 주지 못합니다.


## 두 센서의 차이점과 보완적 역할

각 센서가 가진 단점들을 보완하기 위해 두 센서는 함께 이용되기도 하며 상호보완적인 역할을 합니다.


# Lidar-Camera Fusion 연구 필요성

## 센서 융합이 필요한 이유

Lidar와 카메라는 각각의 장점이 뚜렷하지만, 단독으로 사용했을 때 기술적인 한계가 존재합니다.

예를 들어, **Lidar만 사용한 경우**, 높은 공간 해상도를 제공하지만, 색상 및 텍스처 정보가 없어 물체의 세부적인 특성을 인식하기 어렵습니다. **Camera만 사용한 경우**, 물체의 텍스처나 색상은 인식할 수 있지만, 정확한 거리와 크기 정보를 얻지 못합니다. 

이러한 한계를 극복하기 위해 Lidar-Camera Fusion이 필요합니다. 센서 융합을 통해 두 센서의 데이터를 결합하면, 다음과 같은 이점을 얻을 수 있습니다.

- 높은 정밀도의 3D 객체 탐지:
Lidar로부터 깊이와 거리 정보를 받고, 카메라로부터 물체의 세부 정보를 얻어 객체를 더 정밀하게 탐지할 수 있습니다.
- 강건성(Robustness) 향상:
악천후나 조명이 부족한 상황에서 한 센서의 약점을 다른 센서가 보완합니다.
- 멀티모달 데이터 학습:
두 센서에서 얻은 데이터는 서로 상호보완적으로 학습되어 더 강력한 딥러닝 모델을 설계할 수 있습니다.

자율주행 차량, 로봇공학, 드론 등의 응용 분야에서 센서 융합은 환경 인식과 의사결정의 정확도를 크게 향상시키며, 안전성과 효율성을 높이는 데 필수적인 기술입니다.


## Lidar-Camera Fusion 장점과 단점

두 센서를 같이 사용하게 되면 정확도와 강건성이 향상되며, 다양하게 응용 가능합니다.

하지만, 두 센서를 같이 사용하게 되면 camera만 사용하는 것과 비교했을 때 큰 비용이 발생하고, 카메라 시스템의 추가적인 처리 비용이 요구됩니다. 또한 두 센서 간의 정렬(calibration) 및 시간 동기화(synchronization) 문제를 해결해야 합니다. 또한, 두 센서 데이터를 동시에 처리하려면 높은 계산 능력이 필요하며, 실시간 시스템 구현이 어렵습니다.


# 연구 주요 방향

그래서 Lidar-Camera를 함께 사용하는 연구들은 Sensor Fusion 하는 방법과 특징점들 더 잘 사용하여 성능 향상시키기, 강건선 향상, 효율성 개선으로 크게 4가지 방향으로 나뉩니다.


## Sensor Fusion

### Early Fusion

- 두 센서 데이터를 초기 단계에서 결합하여 학습합니다.
- 장점 : 센서 간 정렬을 정확히 처리하면 원시 데이터의 특성을 잘 유지
- 단점 : 정렬 오류 시 성능 저하가 심하며 계산 비용이 높음
- Ex. MV3D(CVPR, 2017), PointFusion(CVPR, 2018), Frustum PointNets(CVPR, 2018)

### Mid-Level Fusion

- 두 센서의 데이터를 각각 독립적으로 처리한 뒤, 중간 단계에서 피처를 융합합니다.
- 장점 : 각 센서의 특성을 독립적으로 학습하며, 결합의 유연성이 높음
- 단점 : 최적의 융합 지점을 설정하는 것이 도전 과제
- Ex. DeepFusion, PointPainting


### Late Fusion

- 두 센서 데이터를 각각 독립적으로 처리한 후, 최종 결과를 결합합니다.
- 장점 : 각 센서의 독립적인 성능 유지
- 단점 : 융합 단계에서 센서 간 시너지 효과가 제한적
- Ex. 

## Feature Extraction. 센서별 특징점 추출 성능 강화


## Robustness. 강건성 향상

- 악천후(비, 안개, 눈) 환경에서 성능 저하를 방지하기 위한 연구
- 도메인 적응(Domain Adaptation) 및 노이즈 제거 기술 도입


## Efficiency. 실시간 처리와 경량화

- 모델 경량화를 통해 실시간 처리 가능성 확보
- GPU/TPU와 같은 하드웨어 가속 기술을 적극 활용
- Ex. PointPillars



# 대표적인 논문 소개

MV3D. PointPainting. DeepFusion, Frustum PointNet.
이후에 다룰 예정.

# 대표적인 Lidar-Camera 데이터셋 

Waymo Open Dataset, NuScenes, KITTI



---
title:  "[내가 궁금함] Multimodal LLM에 대해" 
categories: research
tag: [python, studying, computer vision, cv, llm, language model, 진행중]
date: 2024-08-06
toc: true
sidebar:
    nav: "docs"
last_modified_at: 2024-08-06
---

최근 LLM이 대거 등장하면서 LLM에 흥미가 생겼고 예전에 CNN+RNN으로 vision과 nlp를 써보려했지만 잘 되지 않았던 기억이 있습니다. 최근에는 vision과 함께 쓰일 때 어떻게 같이 쓰이고 성능은 얼마나 되는지 궁금해서 공부하게 됐습니다. (본인 공부 및 기록용)😁

# Multimodal LLM이란?
멀티모달 언어 모델은 **여러 가지 종류의 데이터를 처리하고 통합**할 수 있도록 설계된 AI 시스템입니다. 전통적인 모델이 주로 텍스트에 집중하는 반면, 멀티모달 LLM은 **이미지, 오디오, 심지어 비디오 데이터도 처리하고 이해**할 수 있습니다. 이러한 능력 덕분에 다양한 종류의 입력을 깊고 포괄적으로 이해해야 하는 작업을 수행할 수 있습니다.

# Multimodal LLM의 주요 구성 요소
1. 텍스트 처리: 인간 언어를 이해하고 생성합니다.
2. 이미지 처리: 시각적 콘텐츠를 인식하고 해석합니다.
3. 오디오 처리: 음성과 소리를 분석하고 이해합니다.
4. 비디오 처리: 시각적 및 음성 데이터를 결합하여 동작과 맥락을 이해합니다.

# Multimodal LLM 작동 방법
멀티모달 LLM은 다양한 종류의 데이터를 통합하기 위해 특화된 아키텍처와 기술을 사용합니다. 작동 방식을 간단히 설명하자면 다음과 같습니다:

## 데이터 인코딩

## 융합 매커니즘

## 멀티모달 이해

## 작업별 헤드

# Multimodal LLM 응용분야
멀티모달 LLM은 다양한 분야에서 많은 가능성을 보여줍니다. 모두가 생각하고 상상해왔던 AI의 모습을 갖추고 있습니다. 
예를 들어 
- 

이것저것 공부하면서 Multi-moal LLM에 대해 새로 알게 되는 내용은 계속 추가할 예정입니다. 궁금한 것들이나 추가 및 수정했으면 좋겠는 거 말해주시면 좋을 거 같아요.
좋은 하루 보내시길 바래요 :)